public:: true

- ## 背景
- ### 持续语义解析（CSP）
	- 目标是在仅有少量标注样本的情况下，使解析器能够将自然语言问题转换为 SQL，并适应动态更新的数据库环境
- ### 主要挑战：
	- **灾难性遗忘**：模型在学习新任务后，对旧任务的表现显著下降。
	- **数据隐私与理想化假设**：现有方法依赖历史数据回放或假设任务标签已知，这在实际场景中可能不现实
- ## 方法
- ### Memory Reconstruction
	- 目标是在**完全不接触真实历史样本**的前提下，搞清楚“新任务到底带来了哪些新的 SQL 语法现象”，从而指导大模型去**有针对性地生成伪样本**来补洞。
	- #### 把 SQL 转成语法骨架
	- 用现成的 SQL 解析器把查询拆成 AST（抽象语法树），把AST的值、表列名、关键词替换为占位符
	- 例：`SELECT name FROM country WHERE population > 1000000 ORDER BY area DESC`
	  骨架：`SELECT <column> FROM <table> WHERE <column> > <value> ORDER BY <column> DESC`
	- #### 构建任务级骨架库
	- 构建当前任务的数据集骨架集合Sₜ，历史任务S₁…St-1，保留骨架频次≥2的避免偶然性错误
	- #### 差异检测
	- **粗粒度**：找出当前任务新出现的完整骨架
	- **细粒度**：把每条骨架拆成**n-gram 语法子树**，如果某子树在历史中从未出现，但当前频繁出现，则标记为“新增子结构”
	- 输出两个差异的集合
	- #### 差异信号 → 提示模板
	- • 把两个差异集合转成自然语言描述，让 LLM 按这些描述生成“可能出现”的自然语言问题+SQL 对作为伪样本。
	- #### 记忆校准(Memory Calibration)
	- 伪样本拿去执行，错了再反馈给LLM，循环3轮取没错的
	- 计算一条 SQL 与其骨架的 **编辑距离**< τ 的保留
- ### Task-Aware Dual-Teacher Distillation
- #### Teacher-LLM (GPT-4 / CodeLlama)
- 当前任务特有知识+历史任务共享知识，经记忆校准后得来 通过**交叉熵损失**让学生拟合
- #### Teacher-Prev (上一轮学生模型)
- 用历史任务共享知识计算**KL散度**，让学生**对齐上一轮自己**在这些样本上的输出，抑制遗忘
-
- [https://github.com/tom68-ll/LECSP](https://github.com/tom68-ll/LECSP)