## GRPO Done Right
- ![Refer to caption](https://arxiv.org/html/2503.20783v1/x1.png)
- [[GRPO]]的目标函数引入了两种偏差：
	- **Response-level length bias**：
		- 源于 GRPO 目标函数中将梯度更新除以响应长度 $$|o_i|$$
		- 优势值$\hat{A}_{i,t}>0$时，GRPO 会尝试增加生成这个响应的概率
			- 如果响应长度小，除数小则梯度更新幅度大
			- 导致模型会觉得“短而精”的正确答案更容易获得大的奖励信号，促使策略给出**正确答案时倾向于更简洁、更短的响应**
		- 优势值$\hat{A}_{i,t}<0$时，GRPO 会尝试降低生成这个响应的概率
			- 如果响应长度大，除数大则梯度更新幅度小
			- 模型会觉得“长而烂”的错误答案受到的惩罚更轻微，导致策略在生成**不正确的响应时倾向于更冗长**
	- **Question-level difficulty bias**：
		- 源于优势计算时除以当前问题生成的所有响应奖励的标准差std
		- 问题太简单，导致奖励都是1，标准差接近0，梯度更新权重非常高
		- 问题太难，导致奖励都是0，标准差同样接近0
		- 导致模型会过度关注这些奖励变动不大的问题(太简单，太难)，其余难度的优化力度不够
- 现有很多框架的PPO实现也存在着长度偏差
- ### 做法
	- 移除掉除以$$|o_i|$$，和优势计算的std
	- 由于计算损失时常会对不同长度的序列进行 padding，并使用 `mask` 来确保只计算真实 token 的损失，`masked_mean` 函数通常会用 `mask.sum()` 来对真实 token 的数量进行归一化
	- DrGRPO将其替换为**常数值**（如预设的生成最大长度），意味着不再根据实际生成的响应长度进行归一化
- ### 实验
	- 对**未经任何特定领域知识预训练的原始 Llama 基础模型**应用 RL带来了微乎其微的 (minimal)提升
	- 通过**持续预训练**在 Llama 模型中**嵌入数学领域知识之后**，模型拥有显著更强的 RL 性能
	- **RLHF 更多的是“精调”或“对齐”模型行为，而不是从零开始教授领域知识**。如果模型缺乏基础的领域知识，RL 的效果就会大打折扣。
	- 认为GRPO 在训练中确实会引入导致模型响应长度不自然增加的偏差，这可能掩盖了模型真实能力，并造成“长 CoT”的假象