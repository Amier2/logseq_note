public:: true

- # [[强化学习基本概念]]
- 参考：
	- [《rlhfbook》](https://rlhfbook.com/)
	- [Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning](https://arxiv.org/html/2503.06072v2#S3)
	-
- # Why RL in NLP?
- 很多 NLP 任务本质是每一步做一个离散决策（选词、选句、选操作），最终拼成一个完整的输出。RL 的「序贯决策」框架天然就可以把「搜索过程」显式地建模成马尔可夫决策过程（MDP）。
- ![image.png](../assets/image_1753689415580_0.png)
- 传统监督学习只能优化「可微的代理损失」（交叉熵、MSE），而真正的评测指标（BLEU、ROUGE、Acc、F1）往往是「不可微」的。RL 的「奖励」可以**直接等于最终指标**，从而“端到端”地优化我们真正关心的目标。
- # RL在NLP的缺陷
- ## 奖励稀疏
- 强化学习的成功依赖于清晰且频繁的奖励信号，NLP的翻译、摘要、对话、代码生成等都依赖**整体质量**（BLEU、ROUGE、单元测试通过率）。只有当整句/整段生成完毕才能计算这些指标，中间每一步都没有可量化的局部奖励，对于代码生成，程序通过全部测试=+1，否则 0。没有“部分正确”的中间奖励。
- 词汇表大小 V，序列长度 L，组合爆炸 V^L；随机探索几乎不可能恰好生成高分句子。
- 没有有效的奖励信号将会导致智能体长期“盲搜”，几乎学不到任何有效梯度。
- ## 探索与利用难以平衡
- 在强化学习中，复杂性和多样性意味着每个模型需要不断地进行大量试错才能够收敛；并且模型会不断探索新的策略，而这些策略往往不总是生成有意义或合适的文本（如乱码、无意义的对话等）。
- 而如果语言模型先验太强，策略很快坍缩成“复读机”或“万能回复”，生成多样性骤降，指标看似高实则过拟合(Reward hacking现象)
- # RL in LLM
- 到目前为止，预训练良好的LLM可以在RL训练时提供可靠探索，模型有了可靠探索能力后通过设计的奖励指标就可以比较有效地向着设计的**偏好**fine tune学习
- ## 后训练
- ![Figure 1: A rendition of the early, three stage RLHF process with SFT, a reward model, and then optimization.](https://rlhfbook.com/c/images/rlhf-basic.png)
- ### 微调(Fine Tuning)
- 微调是将预训练大语言模型（LLM）适配到特定任务的核心环节，它通过有针对性的参数调整来精炼模型能力。该过程利用带有标签或任务专属的数据集来优化性能，解决domain gap。
	- #### 监督微调(Supervised Fine-Tuning)
	- SFT 直接使用标记数据调整模型参数，从而生成既精确又适应上下文的模型，同时保留泛化能力。
	- #### Adaptive Fine-Tuning
	- 该方法引入额外的提示来指导模型生成输出，为定制模型响应提供了灵活的框架。包括**instruction tuning** 和 **prompt-based tuning**
	- #### 强化微调(ReFT) ——OpenAI称之为RFT(注：RFT也可以是拒绝采样微调）
	- **(上面链接中的综述论文中的分类，说白了就是SFT+RL，放在这个FT分类里比较牵强)**
	- 与传统 SFT 仅为每道题提供单一思维链（CoT）标注不同，ReFT 允许模型探索多条有效推理路径，从而增强其泛化能力与解题技巧。
- ### 对齐(Alignment)
- 对齐目的是引导模型输出符合人类期望与偏好，尤其在涉及安全或面向用户的关键场景中。
	- #### 基于人类反馈的强化学习(RLHF)
	- 纯粹SFT中带注释数据的多样性和质量可能参差不齐，且监督模型难以捕捉更细微或更具适应性的人类偏好。为此提出了基于强化学习 (RL) 的微调用于对齐来解决这些缺点。
	- **人类反馈**是 RLHF 的核心，它为奖励模型提供用户偏好信息并指导策略更新。
	- 常见的人类反馈分类有：
		- **评价**：人类对智能体行为的显式评分，通常经二元或多标签标注精炼以降低噪声。
		- **比较**：人类对多条输出或轨迹进行排序；候选集越大，信号越丰富，但也可能带来因果混淆。
		-