public:: true

- ## 发现
- RL只更新了预训练LLM中**一小部分子网络**的参数，而**其余大部分参数几乎保持不变**，这个被更新的子网络仅包含全部参数的 **5% 到 30%**，有时甚至可以低至 **5%**。
- **监督微调 (SFT) 阶段的参数更新则密集得多**
- **所有层和参数矩阵都接收到相似的稀疏但满秩的更新**。只有层归一化参数几乎不更新。
- 通过实验证明了子网络微调可以**恢复到与全量微调几乎完全相同的模型参数值**
- **对于给定的基础模型，跨不同的随机种子、训练数据和RL算法，观察到远高于随机猜测的子网络重叠**。这表明可能存在一个跨不同训练设置的、**一致且至少部分可迁移**的子网络结构
- 当模型在来自与其当前策略分布接近的数据上学习时，策略需要的改变较少，从而参数更新也较少，如PPO, GRPO, PRIME会在线从演变的策略中采样数据，这就是在分布内训练
- 发现**在分布内数据上训练是更新稀疏性的主要驱动因素，这不仅适用于RL，也适用于SFT**
-